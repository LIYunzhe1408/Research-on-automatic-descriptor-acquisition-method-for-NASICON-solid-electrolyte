{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177ec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    Un seul exemple de train / test pour une classification simple des séquences.\n",
    "\n",
    "    Args:\n",
    "        guid: Identifiant pour l'exemple.\n",
    "        text_a: string. Le texte non tokenisé de la première séquence. Pour les séquences seules, \n",
    "        seule cette séquence doit être spécifiée.\n",
    "        label\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, label):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    Un seul ensemble de features de données.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Indices de la séquence d'entrée.\n",
    "        attention_mask: Masque pour éviter d'appliquer de l'attention sur les tokens de padding.\n",
    "            Valeur de masque sont ``[0, 1]``:\n",
    "            ``1`` pour les tokens qui ne sont pas masqués, ``0`` pour les tokens masqués.\n",
    "        token_type_ids: Index de token de segment pour indiquer la première et la deuxième partie des entrées.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_ids, attention_mask, token_type_ids, label_id, e1_mask, e2_mask\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_id = label_id\n",
    "        self.e1_mask = e1_mask\n",
    "        self.e2_mask = e2_mask\n",
    "\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class process_dataset(object):\n",
    "    \"\"\"Processeur pour le dataset \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.relation_labels = get_label(args)\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            label = self.relation_labels.index(line[0])\n",
    "            if i % 1000 == 0:\n",
    "                logger.info(line)\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: train, dev, test\n",
    "        \"\"\"\n",
    "        file_to_read = None\n",
    "        if mode == \"train\":\n",
    "            file_to_read = self.args.train_file\n",
    "        elif mode == \"dev\":\n",
    "            file_to_read = self.args.dev_file\n",
    "        elif mode == \"test\":\n",
    "            file_to_read = self.args.test_file\n",
    "\n",
    "        logger.info(\n",
    "            \"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read))\n",
    "        )\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(self.args.data_dir, file_to_read)), mode\n",
    "        )\n",
    "\n",
    "processors = {\"semeval\": process_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61eae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    max_seq_len,\n",
    "    tokenizer,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    sequence_a_segment_id=0,\n",
    "    add_sep_token=False,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "      \n",
    "        if \"<e1>\" not in tokens_a:\n",
    "            break\n",
    "        e11_p = tokens_a.index(\"<e1>\") \n",
    "        e12_p = tokens_a.index(\"</e1>\") \n",
    "        e21_p = tokens_a.index(\"<e2>\") \n",
    "        e22_p = tokens_a.index(\"</e2>\") \n",
    "        # Remplace le token \n",
    "        tokens_a[e11_p] = \"$\"\n",
    "        tokens_a[e12_p] = \"$\"\n",
    "        tokens_a[e21_p] = \"#\"\n",
    "        tokens_a[e22_p] = \"#\"\n",
    "\n",
    "        # Ajoute 1 à cause du token [CLS]\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        # Prend en compte [CLS] et [SEP] avec \"- 2\".\n",
    "        if add_sep_token:\n",
    "            special_tokens_count = 2\n",
    "        else:\n",
    "            special_tokens_count = 1\n",
    "        if len(tokens_a) > max_seq_len - special_tokens_count:\n",
    "            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "        tokens = tokens_a\n",
    "        if add_sep_token:\n",
    "            tokens += [sep_token]\n",
    "\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        )\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        e1_mask = [0] * len(attention_mask)\n",
    "        e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "        for i in range(e11_p, e12_p + 1):\n",
    "            e1_mask[i] = 1\n",
    "        for i in range(e21_p, e22_p + 1):\n",
    "            e2_mask[i] = 1\n",
    "\n",
    "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(\n",
    "            len(input_ids), max_seq_len\n",
    "        )\n",
    "        assert (\n",
    "            len(attention_mask) == max_seq_len\n",
    "        ), \"Error with attention mask length {} vs {}\".format(\n",
    "            len(attention_mask), max_seq_len\n",
    "        )\n",
    "        assert (\n",
    "            len(token_type_ids) == max_seq_len\n",
    "        ), \"Error with token type length {} vs {}\".format(\n",
    "            len(token_type_ids), max_seq_len\n",
    "        )\n",
    "\n",
    "        label_id = int(example.label)\n",
    "        if ex_index < 5:\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"guid: %s\" % example.guid)\n",
    "            print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            print(\n",
    "                \"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask])\n",
    "            )\n",
    "            print(\n",
    "                \"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids])\n",
    "            )\n",
    "            print(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "            print(\"e1_mask: %s\" % \" \".join([str(x) for x in e1_mask]))\n",
    "            print(\"e2_mask: %s\" % \" \".join([str(x) for x in e2_mask]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                label_id=label_id,\n",
    "                e1_mask=e1_mask,\n",
    "                e2_mask=e2_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a31b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b488dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "\n",
    "def load_tokenizer(args):\n",
    "    tokenizer = MODEL_CLASSES[args.model_type][2].from_pretrained(\n",
    "        args.model_name_or_path\n",
    "    )\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS}\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bbf910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode):\n",
    "    processor = processors[args.task](args)\n",
    "\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            mode,\n",
    "            args.task,\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            args.max_seq_len,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"dev\":\n",
    "            examples = processor.get_examples(\"dev\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise Exception(\"Seulement train, dev, test est possible\")\n",
    "\n",
    "        features = convert_examples_to_features(\n",
    "            examples, args.max_seq_len, tokenizer, add_sep_token=args.add_sep_token\n",
    "        )\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_token_type_ids = torch.tensor(\n",
    "        [f.token_type_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_e1_mask = torch.tensor(\n",
    "        [f.e1_mask for f in features], dtype=torch.long\n",
    "    )  # ajout masque e1\n",
    "    all_e2_mask = torch.tensor(\n",
    "        [f.e2_mask for f in features], dtype=torch.long\n",
    "    )  # ajout masque e2 \n",
    "\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_attention_mask,\n",
    "        all_token_type_ids,\n",
    "        all_label_ids,\n",
    "        all_e1_mask,\n",
    "        all_e2_mask,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bef371ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (BertModel, BertPreTrainedModel)\n",
    "\n",
    "PRETRAINED_MODEL_MAP = {\n",
    "    'bert': BertModel,\n",
    "}\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0., use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class RBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(RBERT, self).__init__(config)\n",
    "        self.bert = PRETRAINED_MODEL_MAP[args.model_type](config=config)  # Load pretrained bert\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.e1_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.e2_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.label_classifier = FCLayer(config.hidden_size * 3, config.num_labels, args.dropout_rate, use_activation=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)  # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        avg_vector = sum_vector.float() / length_tensor.float() \n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        # Average\n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "\n",
    "        # Dropout -> tanh -> fc_layer\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\n",
    "        e1_h = self.e1_fc_layer(e1_h)\n",
    "        e2_h = self.e2_fc_layer(e2_h)\n",
    "\n",
    "        # Concat -> fc_layer\n",
    "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
    "        logits = self.label_classifier(concat_h)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  \n",
    "\n",
    "        # Softmax\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3decc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def get_label(args):\n",
    "    return [\n",
    "        label.strip()\n",
    "        for label in open(\n",
    "            os.path.join(args.data_dir, args.label_file), \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def write_prediction(args, output_file, preds):\n",
    "    relation_labels = get_label(args)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, pred in enumerate(preds):\n",
    "            f.write(\"{}\\t{}\\n\".format(8001 + idx, relation_labels[pred]))\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, RBERT, BertTokenizer),\n",
    "}\n",
    "MODEL_PATH_MAP = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "}\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(preds, labels, average=\"micro\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea965ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.label_lst = get_label(args)\n",
    "     \n",
    "        self.num_labels = len(self.label_lst)\n",
    "\n",
    "        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "        self.config = self.config_class.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            num_labels=self.num_labels,\n",
    "            finetuning_task=args.task,\n",
    "        )\n",
    "        self.model = self.model_class.from_pretrained(\n",
    "            args.model_name_or_path, config=self.config, args=args\n",
    "        )\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = (\n",
    "            #\"cpu\"\n",
    "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "        )\n",
    "\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = (\n",
    "                self.args.max_steps\n",
    "                // (len(train_dataloader) // self.args.gradient_accumulation_steps)\n",
    "                + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = (\n",
    "                len(train_dataloader)\n",
    "                // self.args.gradient_accumulation_steps\n",
    "                * self.args.num_train_epochs\n",
    "            )\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.args.learning_rate,\n",
    "            eps=self.args.adam_epsilon,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.args.warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "        loss_train = []\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "                \n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3],\n",
    "                    \"e1_mask\": batch[4],\n",
    "                    \"e2_mask\": batch[5],\n",
    "                }\n",
    "               \n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "                \n",
    "                \n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "                loss_train.append(loss.item())\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), self.args.max_grad_norm\n",
    "                    )\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if (\n",
    "                        self.args.logging_steps > 0\n",
    "                        and global_step % self.args.logging_steps == 0\n",
    "                    ):\n",
    "                        self.evaluate(\"test\")\n",
    "\n",
    "                    if (\n",
    "                        self.args.save_steps > 0\n",
    "                        and global_step % self.args.save_steps == 0\n",
    "                    ):\n",
    "                        self.save_model()\n",
    "\n",
    "                if 0 < self.args.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if 0 < self.args.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step, loss_train\n",
    "\n",
    "    def evaluate(self, mode):\n",
    "        # We use test dataset because semeval doesn't have dev dataset\n",
    "        if mode == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == \"dev\":\n",
    "            dataset = self.dev_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(\n",
    "            dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Eval!\n",
    "        print(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        print(\"  Num examples = %d\", len(dataset))\n",
    "        print(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        self.model.eval()\n",
    "        loss_eval = []\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3],\n",
    "                    \"e1_mask\": batch[4],\n",
    "                    \"e2_mask\": batch[5],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "                loss_eval.append(tmp_eval_loss.mean().item())\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(\n",
    "                    out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n",
    "                )\n",
    "\n",
    "            \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\"loss\": eval_loss}\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        write_prediction(\n",
    "            self.args, os.path.join(self.args.eval_dir, \"lyz_proposed_answers.txt\"), preds\n",
    "        )\n",
    "\n",
    "        result = compute_metrics(preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"  {} = {:.4f}\".format(key, results[key]))\n",
    "\n",
    "        return results, loss_eval, preds, out_label_ids\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            os.makedirs(self.args.model_dir)\n",
    "        model_to_save = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )\n",
    "        model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "        try:\n",
    "            self.args = torch.load(\n",
    "                os.path.join(self.args.model_dir, \"training_args.bin\")\n",
    "            )\n",
    "            self.config = self.config_class.from_pretrained(self.args.model_dir)\n",
    "            self.model = self.model_class.from_pretrained(\n",
    "                self.args.model_dir, config=self.config, args=self.args\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            logger.info(\"***** Model Loaded *****\")\n",
    "        except:\n",
    "            raise Exception(\"Some model files might be missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7210fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import argparse\n",
    "    import logging\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=\"semeval\", type=str, help=\"The name of the task to train\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=\"./data_RE\",\n",
    "        type=str,\n",
    "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", default=\"./model\", type=str, help=\"Path to model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_dir\",\n",
    "        default=\"./eval\",\n",
    "        type=str,\n",
    "        help=\"Evaluation script, result directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_file\", default=\"train.tsv\", type=str, help=\"Train file\"\n",
    "    )\n",
    "    parser.add_argument(\"--test_file\", default=\"lyz_test.tsv\", type=str, help=\"Test file\")\n",
    "    parser.add_argument(\n",
    "        \"--label_file\", default=\"label.txt\", type=str, help=\"Label file\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=\"bert\",\n",
    "        type=str,\n",
    "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42, help=\"random seed for initialization\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", default=16, type=int, help=\"Batch size for training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\", default=32, type=int, help=\"Batch size for evaluation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_len\",\n",
    "        default=384,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        default=0.0001,\n",
    "        type=float,\n",
    "        help=\"The initial learning rate for Adam.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        default=5.0,\n",
    "        type=float,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_epsilon\", default=0.00001, type=float, help=\"Epsilon for Adam optimizer.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout_rate\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\"Dropout for fully-connected layers\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--logging_steps\", type=int, default=250, help=\"Log every X updates steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=250,\n",
    "        help=\"Save checkpoint every X updates steps.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--do_train\", action=\"store_true\", help=\"Whether to run training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--add_sep_token\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Add [SEP] token at the end of the sentence\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "   \n",
    "\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a0537c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_RE/test\\\\./data_RE/label.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\3879872967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\864295006.py\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[1;34m(args, tokenizer, mode)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     cached_features_file = os.path.join(\n\u001b[0;32m      5\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\752738492.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelation_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\1121962742.py\u001b[0m in \u001b[0;36mget_label\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         for label in open(\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         )\n\u001b[0;32m      8\u001b[0m     ]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_RE/test\\\\./data_RE/label.txt'"
     ]
    }
   ],
   "source": [
    "args.data_dir=\"./data_RE/test\"\n",
    "tokenizer = load_tokenizer(args)\n",
    "\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c48795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing RBERT: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing RBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['label_classifier.linear.bias', 'e1_fc_layer.linear.weight', 'cls_fc_layer.linear.weight', 'e1_fc_layer.linear.bias', 'cls_fc_layer.linear.bias', 'e2_fc_layer.linear.bias', 'label_classifier.linear.weight', 'e2_fc_layer.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation on %s dataset ***** test\n",
      "  Num examples = %d 489\n",
      "  Batch size = %d 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|████▍                                                                  | 1/16 [00:02<00:44,  2.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\3191760051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_eval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23508\\2559953225.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[0mtmp_eval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0meval_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtmp_eval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mloss_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_eval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mnb_eval_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args, test_dataset=test_dataset)\n",
    "args.do_eval = True\n",
    "if args.do_eval:\n",
    "    trainer.load_model()\n",
    "    res, test_loss, preds, labels  = trainer.evaluate(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Jonas] *",
   "language": "python",
   "name": "conda-env-Jonas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
