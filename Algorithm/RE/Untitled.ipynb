{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177ec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    Un seul exemple de train / test pour une classification simple des séquences.\n",
    "\n",
    "    Args:\n",
    "        guid: Identifiant pour l'exemple.\n",
    "        text_a: string. Le texte non tokenisé de la première séquence. Pour les séquences seules, \n",
    "        seule cette séquence doit être spécifiée.\n",
    "        label\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, label):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    Un seul ensemble de features de données.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Indices de la séquence d'entrée.\n",
    "        attention_mask: Masque pour éviter d'appliquer de l'attention sur les tokens de padding.\n",
    "            Valeur de masque sont ``[0, 1]``:\n",
    "            ``1`` pour les tokens qui ne sont pas masqués, ``0`` pour les tokens masqués.\n",
    "        token_type_ids: Index de token de segment pour indiquer la première et la deuxième partie des entrées.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_ids, attention_mask, token_type_ids, label_id, e1_mask, e2_mask\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_id = label_id\n",
    "        self.e1_mask = e1_mask\n",
    "        self.e2_mask = e2_mask\n",
    "\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "class process_dataset(object):\n",
    "    \"\"\"Processeur pour le dataset \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.relation_labels = get_label(args)\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            label = self.relation_labels.index(line[0])\n",
    "            if i % 1000 == 0:\n",
    "                logger.info(line)\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: train, dev, test\n",
    "        \"\"\"\n",
    "        file_to_read = None\n",
    "        if mode == \"train\":\n",
    "            file_to_read = self.args.train_file\n",
    "        elif mode == \"dev\":\n",
    "            file_to_read = self.args.dev_file\n",
    "        elif mode == \"test\":\n",
    "            file_to_read = self.args.test_file\n",
    "\n",
    "        logger.info(\n",
    "            \"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read))\n",
    "        )\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(self.args.data_dir, file_to_read)), mode\n",
    "        )\n",
    "\n",
    "processors = {\"semeval\": process_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    max_seq_len,\n",
    "    tokenizer,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    sequence_a_segment_id=0,\n",
    "    add_sep_token=False,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "      \n",
    "        if \"<e1>\" not in tokens_a:\n",
    "            break\n",
    "        e11_p = tokens_a.index(\"<e1>\") \n",
    "        e12_p = tokens_a.index(\"</e1>\") \n",
    "        e21_p = tokens_a.index(\"<e2>\") \n",
    "        e22_p = tokens_a.index(\"</e2>\") \n",
    "        # Remplace le token \n",
    "        tokens_a[e11_p] = \"$\"\n",
    "        tokens_a[e12_p] = \"$\"\n",
    "        tokens_a[e21_p] = \"#\"\n",
    "        tokens_a[e22_p] = \"#\"\n",
    "\n",
    "        # Ajoute 1 à cause du token [CLS]\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        # Prend en compte [CLS] et [SEP] avec \"- 2\".\n",
    "        if add_sep_token:\n",
    "            special_tokens_count = 2\n",
    "        else:\n",
    "            special_tokens_count = 1\n",
    "        if len(tokens_a) > max_seq_len - special_tokens_count:\n",
    "            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "        tokens = tokens_a\n",
    "        if add_sep_token:\n",
    "            tokens += [sep_token]\n",
    "\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        )\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        e1_mask = [0] * len(attention_mask)\n",
    "        e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "        for i in range(e11_p, e12_p + 1):\n",
    "            e1_mask[i] = 1\n",
    "        for i in range(e21_p, e22_p + 1):\n",
    "            e2_mask[i] = 1\n",
    "\n",
    "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(\n",
    "            len(input_ids), max_seq_len\n",
    "        )\n",
    "        assert (\n",
    "            len(attention_mask) == max_seq_len\n",
    "        ), \"Error with attention mask length {} vs {}\".format(\n",
    "            len(attention_mask), max_seq_len\n",
    "        )\n",
    "        assert (\n",
    "            len(token_type_ids) == max_seq_len\n",
    "        ), \"Error with token type length {} vs {}\".format(\n",
    "            len(token_type_ids), max_seq_len\n",
    "        )\n",
    "\n",
    "        label_id = int(example.label)\n",
    "        if ex_index < 5:\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"guid: %s\" % example.guid)\n",
    "            print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            print(\n",
    "                \"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask])\n",
    "            )\n",
    "            print(\n",
    "                \"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids])\n",
    "            )\n",
    "            print(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "            print(\"e1_mask: %s\" % \" \".join([str(x) for x in e1_mask]))\n",
    "            print(\"e2_mask: %s\" % \" \".join([str(x) for x in e2_mask]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                label_id=label_id,\n",
    "                e1_mask=e1_mask,\n",
    "                e2_mask=e2_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a31b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b488dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "\n",
    "def load_tokenizer(args):\n",
    "    tokenizer = MODEL_CLASSES[args.model_type][2].from_pretrained(\n",
    "        args.model_name_or_path\n",
    "    )\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS}\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbf910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode):\n",
    "    processor = processors[args.task](args)\n",
    "\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            mode,\n",
    "            args.task,\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            args.max_seq_len,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"dev\":\n",
    "            examples = processor.get_examples(\"dev\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise Exception(\"Seulement train, dev, test est possible\")\n",
    "\n",
    "        features = convert_examples_to_features(\n",
    "            examples, args.max_seq_len, tokenizer, add_sep_token=args.add_sep_token\n",
    "        )\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_token_type_ids = torch.tensor(\n",
    "        [f.token_type_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_e1_mask = torch.tensor(\n",
    "        [f.e1_mask for f in features], dtype=torch.long\n",
    "    )  # ajout masque e1\n",
    "    all_e2_mask = torch.tensor(\n",
    "        [f.e2_mask for f in features], dtype=torch.long\n",
    "    )  # ajout masque e2 \n",
    "\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_attention_mask,\n",
    "        all_token_type_ids,\n",
    "        all_label_ids,\n",
    "        all_e1_mask,\n",
    "        all_e2_mask,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef371ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (BertModel, BertPreTrainedModel)\n",
    "\n",
    "PRETRAINED_MODEL_MAP = {\n",
    "    'bert': BertModel,\n",
    "}\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0., use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class RBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(RBERT, self).__init__(config)\n",
    "        self.bert = PRETRAINED_MODEL_MAP[args.model_type](config=config)  # Load pretrained bert\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.e1_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.e2_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
    "        self.label_classifier = FCLayer(config.hidden_size * 3, config.num_labels, args.dropout_rate, use_activation=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)  # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        avg_vector = sum_vector.float() / length_tensor.float() \n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        # Average\n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "\n",
    "        # Dropout -> tanh -> fc_layer\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\n",
    "        e1_h = self.e1_fc_layer(e1_h)\n",
    "        e2_h = self.e2_fc_layer(e2_h)\n",
    "\n",
    "        # Concat -> fc_layer\n",
    "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
    "        logits = self.label_classifier(concat_h)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  \n",
    "\n",
    "        # Softmax\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3decc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def get_label(args):\n",
    "    return [\n",
    "        label.strip()\n",
    "        for label in open(\n",
    "            os.path.join(args.data_dir, args.label_file), \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def write_prediction(args, output_file, preds):\n",
    "    relation_labels = get_label(args)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, pred in enumerate(preds):\n",
    "            f.write(\"{}\\t{}\\n\".format(8001 + idx, relation_labels[pred]))\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, RBERT, BertTokenizer),\n",
    "}\n",
    "MODEL_PATH_MAP = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "}\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(preds, labels, average=\"micro\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea965ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.label_lst = get_label(args)\n",
    "     \n",
    "        self.num_labels = len(self.label_lst)\n",
    "\n",
    "        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "        self.config = self.config_class.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            num_labels=self.num_labels,\n",
    "            finetuning_task=args.task,\n",
    "        )\n",
    "        self.model = self.model_class.from_pretrained(\n",
    "            args.model_name_or_path, config=self.config, args=args\n",
    "        )\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = (\n",
    "            #\"cpu\"\n",
    "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "        )\n",
    "\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = (\n",
    "                self.args.max_steps\n",
    "                // (len(train_dataloader) // self.args.gradient_accumulation_steps)\n",
    "                + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = (\n",
    "                len(train_dataloader)\n",
    "                // self.args.gradient_accumulation_steps\n",
    "                * self.args.num_train_epochs\n",
    "            )\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.args.learning_rate,\n",
    "            eps=self.args.adam_epsilon,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.args.warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "        loss_train = []\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "                \n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3],\n",
    "                    \"e1_mask\": batch[4],\n",
    "                    \"e2_mask\": batch[5],\n",
    "                }\n",
    "               \n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "                \n",
    "                \n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "                loss_train.append(loss.item())\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), self.args.max_grad_norm\n",
    "                    )\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if (\n",
    "                        self.args.logging_steps > 0\n",
    "                        and global_step % self.args.logging_steps == 0\n",
    "                    ):\n",
    "                        self.evaluate(\"test\")\n",
    "\n",
    "                    if (\n",
    "                        self.args.save_steps > 0\n",
    "                        and global_step % self.args.save_steps == 0\n",
    "                    ):\n",
    "                        self.save_model()\n",
    "\n",
    "                if 0 < self.args.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if 0 < self.args.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step, loss_train\n",
    "\n",
    "    def evaluate(self, mode):\n",
    "        # We use test dataset because semeval doesn't have dev dataset\n",
    "        if mode == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == \"dev\":\n",
    "            dataset = self.dev_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(\n",
    "            dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Eval!\n",
    "        print(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        print(\"  Num examples = %d\", len(dataset))\n",
    "        print(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        self.model.eval()\n",
    "        loss_eval = []\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3],\n",
    "                    \"e1_mask\": batch[4],\n",
    "                    \"e2_mask\": batch[5],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "                loss_eval.append(tmp_eval_loss.mean().item())\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(\n",
    "                    out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n",
    "                )\n",
    "\n",
    "            \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\"loss\": eval_loss}\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        write_prediction(\n",
    "            self.args, os.path.join(self.args.eval_dir, \"lyz_proposed_answers.txt\"), preds\n",
    "        )\n",
    "\n",
    "        result = compute_metrics(preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"  {} = {:.4f}\".format(key, results[key]))\n",
    "\n",
    "        return results, loss_eval, preds, out_label_ids\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            os.makedirs(self.args.model_dir)\n",
    "        model_to_save = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )\n",
    "        model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "        try:\n",
    "            self.args = torch.load(\n",
    "                os.path.join(self.args.model_dir, \"training_args.bin\")\n",
    "            )\n",
    "            self.config = self.config_class.from_pretrained(self.args.model_dir)\n",
    "            self.model = self.model_class.from_pretrained(\n",
    "                self.args.model_dir, config=self.config, args=self.args\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            logger.info(\"***** Model Loaded *****\")\n",
    "        except:\n",
    "            raise Exception(\"Some model files might be missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7210fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import argparse\n",
    "    import logging\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=\"semeval\", type=str, help=\"The name of the task to train\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=\"./data_RE/test\",\n",
    "        type=str,\n",
    "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", default=\"./model\", type=str, help=\"Path to model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_dir\",\n",
    "        default=\"./eval\",\n",
    "        type=str,\n",
    "        help=\"Evaluation script, result directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_file\", default=\"train.tsv\", type=str, help=\"Train file\"\n",
    "    )\n",
    "    parser.add_argument(\"--test_file\", default=\"test.tsv\", type=str, help=\"Test file\")\n",
    "    parser.add_argument(\n",
    "        \"--label_file\", default=\"label.txt\", type=str, help=\"Label file\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=\"bert\",\n",
    "        type=str,\n",
    "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42, help=\"random seed for initialization\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", default=16, type=int, help=\"Batch size for training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\", default=32, type=int, help=\"Batch size for evaluation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_len\",\n",
    "        default=384,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        default=0.0001,\n",
    "        type=float,\n",
    "        help=\"The initial learning rate for Adam.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        default=5.0,\n",
    "        type=float,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_epsilon\", default=0.00001, type=float, help=\"Epsilon for Adam optimizer.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout_rate\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\"Dropout for fully-connected layers\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--logging_steps\", type=int, default=250, help=\"Log every X updates steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=250,\n",
    "        help=\"Save checkpoint every X updates steps.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--do_train\", action=\"store_true\", help=\"Whether to run training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--add_sep_token\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Add [SEP] token at the end of the sentence\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "   \n",
    "\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0537c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    695\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_new_proxy_conn\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mhttp_tunnel_required\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 696\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_proxy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    697\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_prepare_proxy\u001B[1;34m(self, conn)\u001B[0m\n\u001B[0;32m    963\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 964\u001B[1;33m         \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    965\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    363\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtls_in_tls_required\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 364\u001B[1;33m                 \u001B[0mconn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_connect_tls_proxy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhostname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    365\u001B[0m                 \u001B[0mtls_in_tls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36m_connect_tls_proxy\u001B[1;34m(self, hostname, conn)\u001B[0m\n\u001B[0;32m    506\u001B[0m             \u001B[0mserver_hostname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhostname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 507\u001B[1;33m             \u001B[0mssl_context\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mssl_context\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    508\u001B[0m         )\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001B[0m in \u001B[0;36mssl_wrap_socket\u001B[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001B[0m\n\u001B[0;32m    452\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 453\u001B[1;33m         \u001B[0mssl_sock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_ssl_wrap_socket_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtls_in_tls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    454\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mssl_sock\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001B[0m in \u001B[0;36m_ssl_wrap_socket_impl\u001B[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001B[0m\n\u001B[0;32m    494\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 495\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mssl_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrap_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\ssl.py\u001B[0m in \u001B[0;36mwrap_socket\u001B[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[0;32m    422\u001B[0m             \u001B[0mcontext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 423\u001B[1;33m             \u001B[0msession\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    424\u001B[0m         )\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\ssl.py\u001B[0m in \u001B[0;36m_create\u001B[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[0;32m    869\u001B[0m                         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 870\u001B[1;33m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_handshake\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    871\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mOSError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\ssl.py\u001B[0m in \u001B[0;36mdo_handshake\u001B[1;34m(self, block)\u001B[0m\n\u001B[0;32m   1138\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msettimeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1139\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_handshake\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1140\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 0] Error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    448\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 449\u001B[1;33m                     \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    450\u001B[0m                 )\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    755\u001B[0m             retries = retries.increment(\n\u001B[1;32m--> 756\u001B[1;33m                 \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merror\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_pool\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_stacktrace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    757\u001B[0m             )\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\urllib3\\util\\retry.py\u001B[0m in \u001B[0;36mincrement\u001B[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[0;32m    573\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mnew_retry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_exhausted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 574\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mMaxRetryError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_pool\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merror\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mResponseError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcause\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    575\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mProxyError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8596\\3862111508.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_tokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mtest_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_and_cache_examples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"test\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8596\\1213452533.py\u001B[0m in \u001B[0;36mload_tokenizer\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mload_tokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     tokenizer = MODEL_CLASSES[args.model_type][2].from_pretrained(\n\u001B[1;32m----> 5\u001B[1;33m         \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     )\n\u001B[0;32m      7\u001B[0m     tokenizer.add_special_tokens(\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1746\u001B[0m                     \u001B[0m_raise_exceptions_for_missing_entries\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1747\u001B[0m                     \u001B[0m_raise_exceptions_for_connection_errors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1748\u001B[1;33m                     \u001B[0m_commit_hash\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcommit_hash\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1749\u001B[0m                 )\n\u001B[0;32m   1750\u001B[0m                 \u001B[0mcommit_hash\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_commit_hash\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresolved_vocab_files\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfile_id\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcommit_hash\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\transformers\\utils\\hub.py\u001B[0m in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001B[0m\n\u001B[0;32m    418\u001B[0m             \u001B[0mresume_download\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mresume_download\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    419\u001B[0m             \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 420\u001B[1;33m             \u001B[0mlocal_files_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    421\u001B[0m         )\n\u001B[0;32m    422\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[0;32m   1055\u001B[0m                     \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1056\u001B[0m                     \u001B[0mproxies\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1057\u001B[1;33m                     \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0metag_timeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1058\u001B[0m                 )\n\u001B[0;32m   1059\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mEntryNotFoundError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mhttp_error\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36mget_hf_file_metadata\u001B[1;34m(url, use_auth_token, proxies, timeout)\u001B[0m\n\u001B[0;32m   1355\u001B[0m         \u001B[0mfollow_relative_redirects\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1356\u001B[0m         \u001B[0mproxies\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1357\u001B[1;33m         \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1358\u001B[0m     )\n\u001B[0;32m   1359\u001B[0m     \u001B[0mhf_raise_for_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    404\u001B[0m             \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    405\u001B[0m             \u001B[0mfollow_relative_redirects\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 406\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    407\u001B[0m         )\n\u001B[0;32m    408\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    440\u001B[0m         \u001B[0mretry_on_status_codes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m         \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 442\u001B[1;33m         \u001B[1;33m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    443\u001B[0m     )\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001B[0m in \u001B[0;36mhttp_backoff\u001B[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mnb_tries\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mmax_retries\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m         \u001B[1;31m# Sleep for X seconds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001B[0m in \u001B[0;36mhttp_backoff\u001B[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m             \u001B[1;31m# Perform request and return if status_code is not in the retry list.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 105\u001B[1;33m             \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    106\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mretry_on_status_codes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\requests\\api.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    540\u001B[0m         }\n\u001B[0;32m    541\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 542\u001B[1;33m         \u001B[0mresp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    543\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    653\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    654\u001B[0m         \u001B[1;31m# Send the request\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 655\u001B[1;33m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    656\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    657\u001B[0m         \u001B[1;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    508\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    509\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreason\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_ProxyError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 510\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mProxyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    511\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreason\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_SSLError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mProxyError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(args)\n",
    "\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c48795",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8596\\3191760051.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTrainer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_dataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_eval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_eval\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mres\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpreds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m  \u001B[1;33m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args, test_dataset=test_dataset)\n",
    "args.do_eval = True\n",
    "if args.do_eval:\n",
    "    trainer.load_model()\n",
    "    res, test_loss, preds, labels  = trainer.evaluate(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Jonas] *",
   "language": "python",
   "name": "conda-env-Jonas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}