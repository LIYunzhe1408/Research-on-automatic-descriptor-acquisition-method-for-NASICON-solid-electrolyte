{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6528213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torchcrf as CRF\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import BertPreTrainedModel, BertModel, BertTokenizer, BertConfig, AdamW\n",
    "\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from seqeval.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5212d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "# seed = 2021\n",
    "# max_len = 75\n",
    "# batch_size = 32\n",
    "# learning_rate = 3e-5\n",
    "# epochs = 20\n",
    "# max_grad_norm = 1.0\n",
    "# test_size = 0.1\n",
    "# earlystopping_patience = 3\n",
    "model_name = \"bert-base-cased\"\n",
    "checkpoint_path = \"add-lstm-crf-checkpoint.cpt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b76be539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus1(data_dir):\n",
    "    \"\"\"获得句子及标签信息\"\"\"\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    data = pd.read_csv(data_dir, header=None, encoding='utf-8')\n",
    "    word = data.iloc[:, 0]\n",
    "    tag = data.iloc[:, 2]\n",
    "    for i in range(len(word)):\n",
    "        if word[i] != \" \":\n",
    "            words.append(word[i])\n",
    "            tags.append(tag[i])\n",
    "        elif word[i] == \" \":\n",
    "            word_lists.append(words)\n",
    "            tag_lists.append(tags)\n",
    "            words = []\n",
    "            tags = []\n",
    "    return word_lists, tag_lists\n",
    "\n",
    "def build_corpus2(data_dir):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(data_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                word = line.split(\" \")[0]\n",
    "                tag = line.split(\" \")[1].strip(\"\\n\")\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                word_lists.append(words)\n",
    "                tag_lists.append(tags)\n",
    "                words = []\n",
    "                tags = []\n",
    "    \n",
    "    return word_lists, tag_lists\n",
    "\n",
    "# df = pd.read_csv(\"./data/DA对比实验数据/top_1k_augmented_sentences.csv\", header=None, encoding=\"utf-8\")\n",
    "# sentences, labels = build_corpus1(\"./data/DA对比实验数据/top_1k_augmented_sentences.csv\")\n",
    "# sentences, labels = build_corpus2(\"./mat_data.txt\")\n",
    "df = pd.read_csv(\"./data_aug.csv\", header=None, encoding=\"utf-8\")\n",
    "sentences, labels = build_corpus1(\"./data_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09914d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-Processing', 'B-Feature', 'I-Feature', 'B-Application', 'I-Property', 'I-Characterization', 'B-Processing', 'I-Application', 'B-Composition', 'B-Condition', 'B-Characterization', 'I-Condition', 'I-Composition', 'B-Property', 'B-Structure', 'O', 'I-Structure', 'PAD']\n",
      "{'I-Processing': 0, 'B-Feature': 1, 'I-Feature': 2, 'B-Application': 3, 'I-Property': 4, 'I-Characterization': 5, 'B-Processing': 6, 'I-Application': 7, 'B-Composition': 8, 'B-Condition': 9, 'B-Characterization': 10, 'I-Condition': 11, 'I-Composition': 12, 'B-Property': 13, 'B-Structure': 14, 'O': 15, 'I-Structure': 16, 'PAD': 17}\n"
     ]
    }
   ],
   "source": [
    "# Ours 数据集\n",
    "tag_values = list(set(df.iloc[:, 2].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag_values.remove(\" \")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "print(tag_values)\n",
    "print(tag2idx)\n",
    "\n",
    "# ceder 数据集\n",
    "# tag_values.append(\"PAD\")\n",
    "# tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "# print(tag_values)\n",
    "# print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75b1aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "197ae426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_BiLSTM_CRF(BertPreTrainedModel):\n",
    "    def __init__(self, config, need_birnn=False, rnn_dim=128):\n",
    "        super(BERT_BiLSTM_CRF, self).__init__(config)\n",
    "        \n",
    "        self.num_tags = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        out_dim = config.hidden_size\n",
    "        self.need_birnn = need_birnn\n",
    "\n",
    "        # 如果为False，则不要BiLSTM层\n",
    "        if need_birnn:\n",
    "            self.birnn = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            out_dim = rnn_dim*2\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(out_dim, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, tags=None, token_type_ids=None, input_mask=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=input_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        if self.need_birnn:\n",
    "            sequence_output, _ = self.birnn(sequence_output)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.hidden2tag(sequence_output)\n",
    "        \n",
    "        outputs = (emissions,) + outputs[2:]\n",
    "        if tags is not None:\n",
    "            loss = -1*self.crf(emissions, tags, mask=input_mask.byte())\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97f1018d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 3.00 GiB already allocated; 6.91 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33464\\3270706634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    711\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    874\u001b[0m                 \u001b[1;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m                 deserialized_objects[root_key] = torch.storage._TypedStorage(\n\u001b[1;32m--> 876\u001b[1;33m                     \u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m                     dtype=dtype)\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\Jonas\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UntypedStorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 3.00 GiB already allocated; 6.91 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba32d3",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6863de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbb58f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./result_literatures/100.txt',\n",
       " './result_literatures/110.txt',\n",
       " './result_literatures/120.txt',\n",
       " './result_literatures/132.txt',\n",
       " './result_literatures/137.txt',\n",
       " './result_literatures/304.txt',\n",
       " './result_literatures/46.txt',\n",
       " './result_literatures/67.txt',\n",
       " './result_literatures/82.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取所有txt格式文献\n",
    "dir_all = \"./result_literatures/\"\n",
    "all_text_name_list = []\n",
    "for home, dirs, files in os.walk(dir_all):\n",
    "    for filename in files:\n",
    "        fullname = os.path.join(home, filename)\n",
    "        all_text_name_list.append(fullname)\n",
    "all_text_name_list = all_text_name_list[:20]\n",
    "all_text_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b3940bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sigle_sent = []\n",
    "remove_xkh = r\"[(](.*)[)]\" \n",
    "remove_zkh = r\"[\\[](.*)[\\]]\"\n",
    "remove_dkh = r\"[\\{](.*)[\\}]\"\n",
    "remove_ex1 = r\"[\\d\\)]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "add182b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result_literatures/100.txt ======>已完成!\n",
      "./result_literatures/110.txt ======>已完成!\n",
      "./result_literatures/120.txt ======>已完成!\n",
      "./result_literatures/132.txt ======>已完成!\n",
      "./result_literatures/137.txt ======>已完成!\n",
      "./result_literatures/304.txt ======>已完成!\n",
      "./result_literatures/46.txt ======>已完成!\n",
      "./result_literatures/67.txt ======>已完成!\n",
      "./result_literatures/82.txt ======>已完成!\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 处理文本得到单个句子列表\n",
    "count = 0\n",
    "for file in all_text_name_list:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_sent_lines = f.readlines()\n",
    "            sent_size = len(raw_sent_lines)\n",
    "            index = 0\n",
    "            cur_sent = \"\"\n",
    "            while index < sent_size:\n",
    "                cur = raw_sent_lines[index].strip()\n",
    "\n",
    "                if cur != \"\":\n",
    "                    cur_sent += (cur + \" \")\n",
    "                    index += 1\n",
    "\n",
    "                elif cur == \"\" and cur_sent != \"\":\n",
    "                    cur_sent_list = sent_tokenize(cur_sent.strip())\n",
    "                    sigle_sent.extend(cur_sent_list)\n",
    "                    cur_sent = \"\"\n",
    "                    index += 1\n",
    "\n",
    "                else:\n",
    "                    index += 1\n",
    "            count += 1\n",
    "            print(file + \" ======>已完成!\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(count)\n",
    "\n",
    "result_sent = []\n",
    "for sent in sigle_sent:\n",
    "    if len(sent.split(\" \")) <= 8:\n",
    "        continue\n",
    "    new_sent1 = re.sub(remove_xkh, '', sent)\n",
    "    new_sent2 = re.sub(remove_zkh, '', new_sent1)\n",
    "    new_sent3 = re.sub(remove_dkh, '', new_sent2)\n",
    "    new_sent4 = re.sub(remove_ex1, '', new_sent3)\n",
    "    result_sent.append(new_sent4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75b10ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e19c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge = {\"Composition\": {}, \"Structure\": {}, \"Property\": {}, \"Processing\": {}, \n",
    "             \"Feature\": {}, \"Application\": {}, \"Characterization\": {}, \"Condition\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0350e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_sent_to_knowledge(sent):\n",
    "    try:\n",
    "        tokenized_sentence = tokenizer.encode(sent)\n",
    "        input_ids = torch.LongTensor([tokenized_sentence])\n",
    "        input_ids = input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, tags=None)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "    \n",
    "\n",
    "        tmp_test = [new_tokens, new_labels]\n",
    "        now_sentence = \" \".join(tmp_test[0])\n",
    "        descriptor_index = []\n",
    "        for i in range(len(tmp_test[0])):\n",
    "            if tmp_test[1][i][0] == \"B\":\n",
    "                descriptor_index.append(i)\n",
    "        descriptor_index.append(len(tmp_test[0]))\n",
    "        descriptor = []\n",
    "        descriptor_type = []\n",
    "        for i in range(len(descriptor_index)-1):\n",
    "            type_ = tmp_test[1][descriptor_index[i]].split(\"-\")[-1]\n",
    "            descriptor_type.append(type_)\n",
    "            descriptor_word = []\n",
    "            for j in range(descriptor_index[i], descriptor_index[i+1]):\n",
    "                if tmp_test[1][j][0] != \"O\":\n",
    "                    descriptor_word.append(tmp_test[0][j])            \n",
    "            descriptor.append(descriptor_word)\n",
    "        descriptors = [\" \".join(d) for d in descriptor]\n",
    "        for i in range(len(descriptors)):\n",
    "            if descriptors[i] not in knowledge[descriptor_type[i]]:\n",
    "                knowledge[descriptor_type[i]][descriptors[i]] = [now_sentence]\n",
    "            else:\n",
    "                knowledge[descriptor_type[i]][descriptors[i]].append(now_sentence)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0523838a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Composition': {},\n",
       " 'Structure': {},\n",
       " 'Property': {},\n",
       " 'Processing': {},\n",
       " 'Feature': {},\n",
       " 'Application': {},\n",
       " 'Characterization': {},\n",
       " 'Condition': {}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sent in result_sent:\n",
    "    add_new_sent_to_knowledge(sent)\n",
    "knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Jonas] *",
   "language": "python",
   "name": "conda-env-Jonas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
